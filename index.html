<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="utf-8">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href="index_files/css.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="index_files/style.css" media="screen" type="text/css">
    <link rel="stylesheet" href="index_files/print.css" media="print" type="text/css">

<link rel="alternate" hreflang="ja" href="https://shun60s.github.io/">

<meta name="google-site-verification" content="Mx7TbvvnZdL4aHZZrD6Ayf5IpDjPWNt6tG7g2KXDFwQ" />

<title>ディープラーニングと　オーディオや音声の信号処理</title>
<meta property="og:title" content="ディープラーニングとオーディオや音声の信号処理">
<meta property="og:locale" content="ja">
<link rel="canonical" href="https://shun60s.github.io/">
<meta property="og:url" content="https://shun60s.github.io/">
<meta property="og:site_name" content="ディープラーニングとオーディオや音声の信号処理">




  </head>

  <body>
    <header>
      <div class="inner">
        <a href="https://github.com/shun60s">
          <h1>deep learning, signal processing</h1>
        </a>        
      </div>
    </header>


<div id="content-wrapper">
<div class="inner clearfix">
<section id="main-content">


<p>ディープラーニングと、オーディオや音声の信号処理のページです。<br>
These are some repositories of deep learning, audio/speech signal processing.</p>



<h2 id="chainer-notch-filter"><a href="https://github.com/shun60s/chainer-notch-filter/">ノッチフィルタのディープラーニング</a></h2>

<p>演算精度が求められるノッチフィルタをディープラーニングで学習できるかどうか試してみた。<br>
A study of design iir notch filter by deep learning framework chainer</p>

<p></p>



<h2 id="fft-wav-upsampling"><a href="https://github.com/shun60s/FFT-Wav-UpSampling/">サンプリング周波数を2倍に</a></h2>

<p>FFT法を使って　音楽などのWAVファイルのサンプリング周波数を2倍にするプログラム。<br>
A converter of audio wav file samplimg rate to 2 times by FFT method</p>

<p></p>



<h2 id="spectrum"><a href="https://shun60s.github.io/Spectrogram_Autoencoder/">メル尺度のスぺクトログラムとオートエンコーダ</a></h2>

<p>メル尺度のスぺクトログラムの作成、オートエンコーダによる事前学習、などの学習練習用。<br>
A practice of making mel spectrogram, CNN autoencode pre-training, and classifier by deep learning</p>

<p></p>


<h2 id="hmm"><a href="https://shun60s.github.io/HMM/">混合分布のHMM</a></h2>

<p>数字の発話のメル尺度のスぺクトログラムを使って、主成分分析により特徴量の次元数を少なくして、混合分布の隠れマルコフモデル（HMM）を使って識別するもの。練習用。<br>
A practice of Hidden Markov Model with Gaussian mixture emissions </p>

<p></p>


<h2 id="dnn"><a href="https://shun60s.github.io/Wave-DNN/">音声信号のDNN </a></h2>

<p>画像認識ではVGG16など事前学習したものを利用できるが、音声認識用途では少ない。 そこで、音声認識エンジンJuliusのディクテーションキットに含まれるDNNを利用するための特徴量FBANK_D_A_Zを計算するpythonを作ってみた。<br>
A python class to get FBANK_D_A_Z from wave file for use julius dictation kit dnn model </p>

<p></p>


<h2 id="dnn-likelihood"><a href="https://shun60s.github.io/Wave-DNN-likelihood/">音声信号のDNN-HMMの対数尤度の計算 </a></h2>

<p>音声認識エンジンJuliusのディクテーションキットに含まれるDNN-HMMモデルを利用して対数尤度を計算するpythonを作ってみた。<br>
A python class to calculate DNN-HMM model Log-likelihood. </p>

<p></p>


<h2 id="formant"><a href="https://shun60s.github.io/Formant/">音声のホルマントの３Ｄ表示 </a></h2>

<p>LPC(線形予測分析)法によるホルマント周波数とピッチ周波数を推定する簡略的なプログラム。<br>
A simple program of estimate formant and pitch frequecny of speech. </p>

<p></p>


<h2 id="vocal-tube-model"><a href="https://shun60s.github.io/Vocal-Tube-Model/">Vocal tract Tube Model </a></h2>
<p>発声の２管声道モデルの周波数特性と生成波形。<br>
a very simple model of vocal tract by two tube. frequecny response and cross-sectional view (area). </p>

<p></p>




<h2 id="chainer-peak-detect"><a href="https://github.com/shun60s/chainer-peak-detect/">chainerによるピーク検出</a></h2>

<p>ディープラーニングフレームワークのchainerによるピーク位置の推定 <br>
A study of 1D data peak detect by deep learning framework chainer</p>

<p></p>


<h2 id="boston-housing"><a href="https://github.com/shun60s/BostonHousing-GBR-NN/">勾配ブースティング回帰による住宅価格の予測</a></h2>

<p>ボストンハウスのデータ(13種類の指標と住宅価格のデータ）で住宅価格を予測する勾配ブースティング回帰(Gradient Boosting regression)の動作を理解する。ニューラルネットワークで予測した場合と比較する。 <br>
A study of Boston Housing Dataset problem by Gradient Boosting regression model and neural network model</p>

<p></p>


<h2 id="blind-speech-separation"><a href="https://shun60s.github.io/Blind-Speech-Separation/">U-Netによる音楽と音声のミックス信号（モノラル）からの音声の分離</a></h2>
<p>バックに音楽が流れていて、そこから音声だけを抽出するような場面を想定して、音楽と相関のない音声をミックスしたモノラル信号から音声部分を抜き出す実験をしてみた。 <br>
A study of blind speech separation using U-Net</p>
<p></p>



<h2 id="music-tagging-chainer"><a href="https://shun60s.github.io/music-tagging-chainer/">音楽のジャンル分け</a></h2>
<p>Kerasで作成された音楽のジャンル分け（タグ付け）をChainer用に作り変えてみた。 <br>
A remake of Music Genre Classification with Deep Learning</p>
<p></p>


<h2 id="Python-WORLD-Win10"><a href="https://shun60s.github.io/Python-WORLD-Win10/">音声分析変換合成システムWORLDのPython </a></h2>
<p>WORLD PYTHONをWindows10でも動くように変更したもの。 <br>
A change of Python WORLD to function in win10 environment</p>
<p></p>


<h2 id="Wavenet"><a href="https://shun60s.github.io/chainer-examples-wavenet-clone/">WaveNetの実験</a></h2>
<p>chainer-colab-notebookで公開されているWaveNetをGoogle Colaboratoryで実験してみた。<br>
An experiment of WaveNet on Google Colaboratory</p>
<p></p>



<h2 id="softmax-indexi-weighed"><a href="https://shun60s.github.io/softmax-index-weighted/">損失関数の自作</a></h2>
<p>chainerを使ってインデック差で重み付けする損失関数を自作してみた。<br>
A hand-made loss function which uses index difference as weight</p>
<p></p>

<h2 id="spectral-subtraction"><a href="https://github.com/shun60s/spectral-subtraction/">スペクトル・サブトラクション</a></h2>
<p>ノイズ抑制の手法の、簡単なスペクトル・サブトラクション。<br>
A simple spectral subtraction</p>
<p></p>

<h2 id="vocal-tube-noise-s"><a href="https://shun60s.github.io/Vocal-Tube-Noise-S-Model/">摩擦音の「さ」音の生成</a></h2>
<p>２管声道モデルとノイズ源を使った摩擦音の「さ」音の生成<br>
generation of fricative voice /sa/ sound by two tubes model and noise source instead of turbulent sound.</p>
<p></p>

<h2 id="vocal-tube-noise-k"><a href="https://shun60s.github.io/Vocal-Tube-Noise-K-Model/">破裂音の「が」「か」音の生成</a></h2>
<p>爆風インパルス波とノイズ源と２管声道モデルを使った破裂音の「が」「か」音の生成<br>
generation of plosive voice /ga/ /ka/ sound by pseudo blast impulse, noise source instead of turbulent sound, and two tubes model.</p>
<p></p>


<h2 id="vocal-tube-N"><a href="https://shun60s.github.io/Vocal-Tube-N-Model/">鼻音の「な」「ま」音の生成</a></h2>
<p>２管声道モデルと鼻の効果を含む音源を使った鼻音の「な」「ま」音の生成<br>
generation of nasal voice /na/ /ma/ sound by two tubes model and nasal effect source.</p>
<p></p>


<h2 id="vocal-tube-I"><a href="https://shun60s.github.io/Vocal-Tube-I-Model/">ノイズ源有り無しの「い」音の生成</a></h2>
<p>２管声道モデルとノイズ源を使った母音「い」音の生成<br>
generation vowel /i/ sound by two tubes model and noise source.</p>
<p></p>


<h2 id="glottal-spectrum"><a href="https://shun60s.github.io/glottal-source-spectrum/">声門の音源のスペクトルの予想</a></h2>
<p>口の放射特性の逆フィルターとフォルマント周波数で減衰するフィルターを使って声門の音源のスペクトルの状態を予想する<br>
A trial estimation of glottal source spectrum by anti-formant filter and inverse radiation filter.</p>
<p></p>


<h2 id="vocal-tube-estimation"><a href="https://shun60s.github.io/Vocal-Tube-Estimation/">２管声道モデルの推定</a></h2>
<p>周波数特性のピークとドロップピークから２管声道モデルを推定する。<br>
estimation two tube model from peak and drop-peak frequencies of the vowel signal.</p>

<p></p>



<h2 id="DNN meaning">DNNの意味</h2>
<p>非常に多くの変数をもつ連立方程式を、その変数の多さに見合ったサンプル（経験）で解くことによって、（その複雑な）経験を表現（分類）できるようになる。原理的には、対象の複雑さにみあった変数の多さで、ありとあらゆる経験を学習させれば、ハズレは出なくなるようになるのかもしれない。<br>
 （2018年11月8日記）
</p>
<p></p>

</section>


</div>
</div>
<br>
<br>
<br>
<p class="naname1">リンク Link</p>
<figure class="link-box">
<a href="https://qiita.com/terms"><img src="index_files/qiita.png" alt="qiita"></a>
<figcaption>Qiita</figcaption>
</figure>
<figure class="link-box">
<a href="https://github.com"><img src="index_files/github.png" alt="github"></a>
<figcaption>GitHub</figcaption>
</figure>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

</body></html>
