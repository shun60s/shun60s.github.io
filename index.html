<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="utf-8">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href="index_files/css.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="index_files/style.css" media="screen" type="text/css">
    <link rel="stylesheet" href="index_files/print.css" media="print" type="text/css">

<link rel="alternate" hreflang="ja" href="https://shun60s.github.io/">

<meta name="google-site-verification" content="Mx7TbvvnZdL4aHZZrD6Ayf5IpDjPWNt6tG7g2KXDFwQ" />

<title>ディープラーニングと　オーディオや音声の信号処理</title>
<meta property="og:title" content="ディープラーニングとオーディオや音声の信号処理">
<meta property="og:locale" content="ja">
<link rel="canonical" href="https://shun60s.github.io/">
<meta property="og:url" content="https://shun60s.github.io/">
<meta property="og:site_name" content="ディープラーニングとオーディオや音声の信号処理">




  </head>

  <body>
    <header>
      <div class="inner">
        <a href="https://github.com/shun60s">
          <h1>deep learning, signal processing</h1>
        </a>        
      </div>
    </header>


<div id="content-wrapper">
<div class="inner clearfix">
<section id="main-content">


<p>ディープラーニングと、オーディオや音声の信号処理のページです。<br>
These are some repositories of deep learning, audio/speech signal processing.</p>



<h2 id="chainer-notch-filter"><a href="https://github.com/shun60s/chainer-notch-filter/">ノッチフィルタのディープラーニング</a></h2>

<p>演算精度が求められるノッチフィルタをディープラーニングで学習できるかどうか試してみた。<br>
A study of design iir notch filter by deep learning framework chainer</p>

<p></p>



<h2 id="fft-wav-upsampling"><a href="https://github.com/shun60s/FFT-Wav-UpSampling/">サンプリング周波数を2倍に</a></h2>

<p>FFT法を使って　音楽などのWAVファイルのサンプリング周波数を2倍にするプログラム。<br>
A converter of audio wav file samplimg rate to 2 times by FFT method</p>

<p></p>



<h2 id="spectrum"><a href="https://shun60s.github.io/Spectrogram_Autoencoder/">メル尺度のスぺクトログラムとオートエンコーダ</a></h2>

<p>メル尺度のスぺクトログラムの作成、オートエンコーダによる事前学習、などの学習練習用。<br>
A practice of making mel spectrogram, CNN autoencode pre-training, and classifier by deep learning</p>

<p></p>


<h2 id="hmm"><a href="https://shun60s.github.io/HMM/">混合分布のHMM</a></h2>

<p>数字の発話のメル尺度のスぺクトログラムを使って、主成分分析により特徴量の次元数を少なくして、混合分布の隠れマルコフモデル（HMM）を使って識別するもの。練習用。<br>
A practice of Hidden Markov Model with Gaussian mixture emissions </p>

<p></p>


<h2 id="dnn"><a href="https://shun60s.github.io/Wave-DNN">音声信号のDNN </a></h2>

<p>画像認識ではVGG16など事前学習したものを利用できるが、音声認識用途では少ない。 そこで、音声認識エンジンJuliusのディクテーションキットに含まれるDNNを利用するための特徴量FBANK_D_A_Zを計算するpythonを作ってみた。<br>
A python class to get FBANK_D_A_Z from wave file for use julius dictation kit dnn model </p>

<p></p>


<h2 id="dnn-likelihood"><a href="https://shun60s.github.io/Wave-DNN-likelihood">音声信号のDNN-HMMの対数尤度の計算 </a></h2>

<p>音声認識エンジンJuliusのディクテーションキットに含まれるDNN-HMMモデルを利用して対数尤度を計算するpythonを作ってみた。<br>
A python class to calculate DNN-HMM model Log-likelihood. </p>

<p></p>


<h2 id="formant"><a href="https://shun60s.github.io/Formant">音声のホルマントの３Ｄ表示 </a></h2>

<p>LPC(線形予測分析)法によるホルマント周波数とピッチ周波数を推定する簡略的なプログラム。<br>
A simple program of estimate formant and pitch frequecny of speech. </p>

<p></p>


<h2 id="vocal-tube-model"><a href="https://shun60s.github.io/Vocal-Tube-Model">Vocal tract Tube Model </a></h2>
<p>発声の２管声道モデルの周波数特性と生成波形。<br>
a very simple model of vocal tract by two tube. frequecny response and cross-sectional view (area). </p>

<p></p>



<h2 id="formant-to-vocal-tube-model"><a href="https://shun60s.github.io/Formant2TubeModel"> Formant to Vocal Tube Model</a></h2>
<p>ホルマント周波数から２管声道モデルのパラメータを推定する。<br>
estimation vocal tract model, two tube model parameter from formant of voice signal</p>

<p></p>

<h2 id="chainer-peak-detect"><a href="https://github.com/shun60s/chainer-peak-detect/">chainerによるピーク検出</a></h2>

<p>ディープラーニングフレームワークのchainerによるピーク位置の推定 <br>
A study of 1D data peak detect by deep learning framework chainer</p>

<p></p>


<h2 id="boston-housing"><a href="https://github.com/shun60s/BostonHousing-GBR-NN/">勾配ブースティング回帰による住宅価格の予測</a></h2>

<p>ボストンハウスのデータ(13種類の指標と住宅価格のデータ）で住宅価格を予測する勾配ブースティング回帰(Gradient Boosting regression)の動作を理解する。ニューラルネットワークで予測した場合と比較する。 <br>
A study of Boston Housing Dataset problem by Gradient Boosting regression model and neural network model</p>

<p></p>


<h2 id="blind-speech-separation"><a href="https://shun60s.github.io/Blind-Speech-Separation">U-Netによる音楽と音声のミックス信号（モノラル）からの音声の分離</a></h2>
<p>バックに音楽が流れていて、そこから音声だけを抽出するような場面を想定して、音楽と相関のない音声をミックスしたモノラル信号から音声部分を抜き出す実験をしてみた。 <br>
A study of blind speech separation using U-Net</p>
<p></p>



<h2 id="music-tagging-chainer"><a href="https://shun60s.github.io/music-tagging-chainer">音楽のジャンル分け</a></h2>
<p>Kerasで作成された音楽のジャンル分け（タグ付け）をChainer用に作り変えてみた。 <br>
A remake of Music Genre Classification with Deep Learning</p>
<p></p>


<h2 id="DNN meaning">DNNの意味と課題</h2>
<p>非常に多くの変数をもつ連立方程式を、その変数の多さに見合ったサンプル（経験）で解くことによって、（その複雑な）経験を表現（分類）できるようになる。原理的には、対象の複雑さにみあった変数の多さで、ありとあらゆる経験を学習させれば、ハズレは出なくなるようになるのかもしれない。<br>
今後の次ぎの課題は、膨大なサンプルをつかわないで、学習を完成させる方法を考え出すことではないだろうか。 （2018年7月5日記）
</p>
<p></p>

</section>


</div>
</div>
<br>
<br>
<br>
<p class="naname1">リンク Link</p>
<figure class="link-box">
<a href="https://qiita.com/terms"><img src="index_files/qiita.png" alt="qiita"></a>
<figcaption>Qiita</figcaption>
</figure>
<figure class="link-box">
<a href="https://github.com"><img src="index_files/github.png" alt="github"></a>
<figcaption>GitHub</figcaption>
</figure>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

</body></html>
